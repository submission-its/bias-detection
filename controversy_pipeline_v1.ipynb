{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b7b1d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16b7b1d7",
    "outputId": "928b6d0b-06cb-4272-b8df-3d3c79f8fa56"
   },
   "outputs": [],
   "source": [
    "%pip install pymupdf together openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458d130",
   "metadata": {
    "id": "9458d130"
   },
   "outputs": [],
   "source": [
    "from together import Together\n",
    "from openai import OpenAI\n",
    "import fitz\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "MINER_MODEL = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"  \n",
    "\n",
    "COUNCIL_MODELS = [\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    \"openai/gpt-oss-120b\",\n",
    "    \"deepseek-ai/DeepSeek-V3.1\",\n",
    "    \"deepcogito/cogito-v2-1-671b\",\n",
    "    \"moonshotai/Kimi-K2-Thinking\",\n",
    "\n",
    "]\n",
    "\n",
    "META_JURY_MODEL = \"gpt-5.2\"  \n",
    "\n",
    "OUTPUT_FILE = \"results_corvinul.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d592bdcc",
   "metadata": {
    "id": "d592bdcc"
   },
   "outputs": [],
   "source": [
    "def get_total_pages(pdf_path: str) -> int:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = len(doc)\n",
    "    doc.close()\n",
    "    return total_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c6b6c6",
   "metadata": {
    "id": "d4c6b6c6"
   },
   "outputs": [],
   "source": [
    "def pdf_page_to_base64(pdf_path: str, page_num: int, dpi: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Converts PDF page to PNG Base64.\n",
    "    Resizes to max 1280px for better text readability.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[page_num]\n",
    "    \n",
    "    pix = page.get_pixmap(dpi=dpi)\n",
    "    \n",
    "    mode = \"RGBA\" if pix.alpha else \"RGB\"\n",
    "    img = Image.frombytes(mode, [pix.width, pix.height], pix.samples)\n",
    "    \n",
    "    if img.mode in (\"RGBA\", \"LA\", \"P\"):\n",
    "        background = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
    "        if img.mode == \"P\":\n",
    "            img = img.convert(\"RGBA\")\n",
    "        background.paste(img, mask=img.split()[-1] if img.mode == \"RGBA\" else None)\n",
    "        img = background\n",
    "        \n",
    "    max_size = 1280\n",
    "    if max(img.size) > max_size:\n",
    "        ratio = max_size / max(img.size)\n",
    "        new_size = (int(img.width * ratio), int(img.height * ratio))\n",
    "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    buffered = io.BytesIO()\n",
    "    img.save(buffered, format=\"PNG\", optimize=True)\n",
    "    img_bytes = buffered.getvalue()\n",
    "    \n",
    "    return base64.b64encode(img_bytes).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7f0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_json(response_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the first valid JSON object from a string, ignoring \n",
    "    <think> blocks, markdown wrappers, and conversational text.\n",
    "    \"\"\"\n",
    "    clean_text = re.sub(r\"```json\\s*\", \"\", response_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r\"```\", \"\", clean_text)\n",
    "\n",
    "    clean_text = re.sub(r\"<think>.*?</think>\", \"\", clean_text, flags=re.DOTALL)\n",
    "\n",
    "    match = re.search(r'\\{.*\\}', clean_text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        json_str = match.group(0)\n",
    "        try:\n",
    "            return json.loads(json_str, strict=False)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"error\": \"JSON Decode Error\", \"raw_content\": response_text}\n",
    "    else:\n",
    "        return {\"error\": \"No JSON found\", \"raw_content\": response_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7710604d",
   "metadata": {
    "id": "7710604d"
   },
   "outputs": [],
   "source": [
    "def analyze_image(prompt: str, image_b64: str, model: str) -> dict:\n",
    "    \"\"\"Calls vision model with image and returns JSON response.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Analyze this textbook page:\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_b64}\"}}\n",
    "            ]}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    result_text = response.choices[0].message.content.strip()\n",
    "    return json.loads(result_text, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495c134",
   "metadata": {
    "id": "0495c134"
   },
   "outputs": [],
   "source": [
    "def analyze_text(prompt: str, content: str, model: str, **kwargs) -> dict:\n",
    "    \"\"\"\n",
    "    Calls text model and extracts JSON, handling thinking models.\n",
    "    \"\"\"\n",
    "    if \"max_tokens\" not in kwargs:\n",
    "        if \"thinking\" in model.lower() or \"r1\" in model.lower():\n",
    "            kwargs[\"max_tokens\"] = 16000 \n",
    "        else:\n",
    "            kwargs[\"max_tokens\"] = 4096\n",
    "\n",
    "    result_text = \"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": content}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            **kwargs \n",
    "        )\n",
    "        result_text = response.choices[0].message.content.strip()\n",
    "    except Exception:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt},\n",
    "                    {\"role\": \"user\", \"content\": content}\n",
    "                ],\n",
    "                **kwargs \n",
    "            )\n",
    "            result_text = response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"API Error: {str(e)}\"}\n",
    "    \n",
    "    if not result_text:\n",
    "        return {\"error\": \"Empty response from model\", \"raw_response\": \"\"}\n",
    "\n",
    "    data = extract_clean_json(result_text)\n",
    "    if isinstance(data, dict):\n",
    "        data[\"_raw_response\"] = result_text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38cf8f1b",
   "metadata": {
    "id": "38cf8f1b"
   },
   "outputs": [],
   "source": [
    "MINER_PROMPT = \"\"\"You are an expert Educational Auditor performing a preliminary bias scan of a Romanian history textbook for controversial content.\n",
    "\n",
    "You will receive a BATCH of pages. Analyze them TOGETHER.\n",
    "\n",
    "### YOUR CORE MISSION\n",
    "Identify **ANY passage that could influence a student's historical interpretation** in a non-neutral way.\n",
    "\n",
    "This includes:\n",
    "- Subtle framing choices\n",
    "- Value-laden adjectives\n",
    "- Selective emphasis or silence\n",
    "- Normalization of one perspective\n",
    "- Assumptions presented as facts\n",
    "- National, ethnic, political, or moral alignment cues\n",
    "\n",
    "You are NOT deciding whether the content is definitively biased or wrong.\n",
    "You are identifying **POTENTIAL POINTS OF INTERPRETIVE INFLUENCE** that merit further review.\n",
    "\n",
    "### CRITICAL RULE: VERBATIM QUOTE EXTRACTION\n",
    "When you fill the \"quotes\" field in the JSON, you must act as a strict OCR engine:\n",
    "1. **EXACT MATCH:** Copy the text **exactly** as it appears in the image. Do not paraphrase, summarize, or fix typos.\n",
    "2. **NO TRANSLATION:** If the text is in Romanian, quote it in Romanian.\n",
    "3. **PRESERVE STYLE:** Keep the original punctuation and diacritics if visible.\n",
    "4. **CONTEXT:** If the biased text is part of a longer sentence, include enough context so the meaning is clear.\n",
    "\n",
    "### EVIDENTIARY COMPLETENESS REQUIREMENT\n",
    "\n",
    "All interpretive concerns must be supported by direct quotations.\n",
    "\n",
    "In the \"quotes\" field:\n",
    "- Include EVERY passage that materially contributed to identifying the concern.\n",
    "- If the issue depends on multiple sentences, sections, or contrasting passages, include each relevant excerpt as a separate quote entry.\n",
    "- Do NOT rely on unquoted text to justify your explanation.\n",
    "- Do NOT quote only a fragment if the broader sentence or paragraph materially affects meaning.\n",
    "- The explanation must be fully supported by the quoted text.\n",
    "- Copy the text **exactly** as it appears in the image. Do not paraphrase, summarize, or fix typos.\n",
    "- If the text is in Romanian, quote it in Romanian.\n",
    "- Keep the original punctuation and diacritics if visible.\n",
    "\n",
    "If an idea influenced your concern but is not directly quoted, you must add the relevant text to the quotes field.\n",
    "\n",
    "\n",
    "### AUTHOR VS. SOURCE AWARENESS\n",
    "\n",
    "History textbooks often include quotations from earlier periods that reflect the language, values, or prejudices of their time.\n",
    "\n",
    "IMPORTANT:\n",
    "- Offensive or exclusionary language appearing in a **Primary Source** does NOT automatically constitute textbook bias.\n",
    "- Do NOT treat historical quotations as problematic merely for reflecting past attitudes.\n",
    "\n",
    "Your task is to:\n",
    "- Correctly identify whether the language comes from the **Textbook Author** or from a **Historical Source**\n",
    "- Flag the passage ONLY if the way the quote is used (selection, emphasis, framing, or lack of context) could influence interpretation\n",
    "\n",
    "### OUTPUT FORMAT (JSON ONLY)\n",
    "You may use multi-step reasoning internally.\n",
    "Return a JSON object with a list of issues.\n",
    "{\n",
    "  \"issues\": [\n",
    "    {\n",
    "      \"controversy\": \"Short title\",\n",
    "      \"source_type\": \"<TEXTBOOK_NARRATIVE|PRIMARY_SOURCE_USAGE>\",\n",
    "      \"quotes\": [\n",
    "        {\"text\": \"PASTE EXACT TEXT FROM IMAGE HERE\", \"page_offset\": 0}\n",
    "      ],\n",
    "      \"explanation\": \"Explain why this passage could influence interpretation or raise a potential concern. Avoid definitive judgments.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "If no issues found, return:\n",
    "{\"issues\": []}\n",
    "\"\"\"\n",
    "\n",
    "COUNCIL_PROMPT = \"\"\"You are an expert Educational Historian auditing a Romanian high school history textbook.\n",
    "\n",
    "### INPUT DATA\n",
    "- **CLAIM:** {controversy}\n",
    "- **CONTEXT TYPE:** {source_type} (Is this the Author's Narrative or a Primary Source?)\n",
    "- **EVIDENCE:** {quotes}\n",
    "- **MINER'S NOTE:** {explanation}\n",
    "\n",
    "You are encouraged to assign low severity or dismiss concerns when appropriate.\n",
    "\n",
    "### STEP 1: Attribution Assessment\n",
    "Determine whether the interpretive concern arises from:\n",
    "- The **Textbook Author's Narrative**, or\n",
    "- The **Use of a Primary Source**\n",
    "\n",
    "If the passage is a Primary Source:\n",
    "- Assess whether it is contextualized or neutrally presented\n",
    "- Neutral presentation generally corresponds to low severity\n",
    "\n",
    "### STEP 2: Category Classification\n",
    "Select ONE category from the list below that best describes the primary interpretive concern.\n",
    "You MUST use one of these exact strings — do not invent new category names.\n",
    "\n",
    "**Language & Framing**\n",
    "- Linguistic Tone\n",
    "- Narrative Framing\n",
    "- Moral Loading\n",
    "\n",
    "**Perspective & Representation**\n",
    "- Perspective Limitation\n",
    "- Group Representation\n",
    "- Agency Attribution\n",
    "\n",
    "**Structure & Emphasis**\n",
    "- Selection Bias\n",
    "- Omission / Underdevelopment\n",
    "- Asymmetrical Detail\n",
    "\n",
    "**Source Handling**\n",
    "- Primary Source Framing\n",
    "- Attribution Ambiguity\n",
    "- Source Selection Bias\n",
    "\n",
    "**Interpretation & Pedagogy**\n",
    "- Teleological Narrative\n",
    "- National or Cultural Centering\n",
    "- Presentism\n",
    "\n",
    "### STEP 3: Severity Assessment (7-Point Scale)\n",
    "- **1: Neutral / Pedagogically Sound**\n",
    "- **2: Negligible Framing**\n",
    "- **3: Minor Imbalance**\n",
    "- **4: Moderate Bias**\n",
    "- **5: Significant Distortion**\n",
    "- **6: Severe Misrepresentation**\n",
    "- **7: Educational Harm / Propaganda**\n",
    "\n",
    "### STEP 4: Confidence\n",
    "Rate your confidence from 0.0 (uncertain) to 1.0 (certain).\n",
    "\n",
    "### OUTPUT FORMAT (JSON ONLY)\n",
    "{{\n",
    "  \"attribution\": \"<TEXTBOOK_NARRATIVE|PRIMARY_SOURCE_USAGE>\",\n",
    "  \"category\": \"<one exact category string from the Step 2 list>\",\n",
    "  \"severity\": <integer 1-7>,\n",
    "  \"confidence\": <number 0.0-1.0>,\n",
    "  \"reasoning\": \"Concise, outcome-focused justification explaining the assessment.\"\n",
    "}}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "META_JURY_PROMPT = \"\"\"\n",
    "You are the Chief Juror of an Educational Audit Council.\n",
    "\n",
    "Your role is to synthesize multiple expert evaluations into a single authoritative final verdict.\n",
    "You must exercise independent judgment: weigh the juror inputs, identify consensus or disagreement, and produce your own reasoned conclusion.\n",
    "\n",
    "### YOUR GOALS\n",
    "1. Identify consensus or meaningful disagreement across jurors\n",
    "2. Weight evaluations by juror confidence\n",
    "3. Produce a final severity score and system-level confidence\n",
    "4. Flag cases requiring human review when appropriate\n",
    "\n",
    "### INPUT DATA\n",
    "**Original Controversy:** {controversy}  \n",
    "**Source Type:** {source_type}  \n",
    "**Evidence Quotes:** {quotes}  \n",
    "**Miner's Note:** {explanation}  \n",
    "**Individual Juror Evaluations:** {juror_evaluations}\n",
    "\n",
    "### ALLOWED TAXONOMY\n",
    "The final_category MUST be one of the following exact strings:\n",
    "[Linguistic Tone, Narrative Framing, Moral Loading, Perspective Limitation, Group Representation, Agency Attribution, Selection Bias, Omission / Underdevelopment, Asymmetrical Detail, Primary Source Framing, Attribution Ambiguity, Source Selection Bias, Teleological Narrative, National or Cultural Centering, Presentism, INVALID_INPUT]\n",
    "\n",
    "### DECISION LOGIC\n",
    "- **High-Confidence Consensus:** If jurors converge on a similar severity and category with confidence > 0.7, adopt their consensus.\n",
    "\n",
    "- **Resolving Disagreements:** If jurors disagree, do NOT just calculate a blind average. Give more weight to high-confidence jurors. Read their reasoning and select the category and severity that is best supported by the evidence. \n",
    "\n",
    "- **Human Review Flag:** Flag for human review when high-confidence jurors differ by more than 1.5 severity points\n",
    "\n",
    "### OUTPUT FORMAT (JSON ONLY)\n",
    "{{\n",
    "  \"final_attribution\": \"<TEXTBOOK_NARRATIVE|PRIMARY_SOURCE_USAGE>\",\n",
    "  \"final_category\": \"<one exact category string from the ALLOWED TAXONOMY>\",\n",
    "  \"final_severity\": <integer 1-7>,\n",
    "  \"system_confidence\": <number 0.0-1.0>,\n",
    "  \"flag_for_human_review\": <true|false>,\n",
    "  \"synthesis_summary\": \"Concise summary of consensus or disagreement and the rationale for the final verdict.\",\n",
    "  \"variance_note\": \"If flagged for review, briefly describe the nature of the disagreement. If not flagged, set to empty string.\"\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730cc37",
   "metadata": {
    "id": "f730cc37"
   },
   "outputs": [],
   "source": [
    "def run_miner_batch(pdf_path: str, start_page: int, num_pages: int = 10, debug: bool = False) -> list:\n",
    "    \"\"\"Analyzes multiple PDF pages together for controversial content.\"\"\"\n",
    "    if debug:\n",
    "        print(f\"  [miner] pages {start_page+1}–{start_page+num_pages} | model: {MINER_MODEL.split('/')[-1]}\")\n",
    "\n",
    "    message_content = [{\"type\": \"text\", \"text\": \"Analyze these textbook pages together as a batch:\"}]\n",
    "    total_size = 0\n",
    "\n",
    "    for offset in range(num_pages):\n",
    "        page_num = start_page + offset\n",
    "        image_b64 = pdf_page_to_base64(pdf_path, page_num)\n",
    "        total_size += len(image_b64)\n",
    "        message_content.append(\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_b64}\"}}\n",
    "        )\n",
    "\n",
    "    if debug:\n",
    "        avg_size = total_size // num_pages\n",
    "        print(f\"  [miner] {num_pages} pages encoded | avg size: {avg_size:,} chars\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MINER_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": MINER_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": message_content},\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    result_text = response.choices[0].message.content.strip()\n",
    "    result = json.loads(result_text, strict=False)\n",
    "\n",
    "    if isinstance(result, dict) and \"issues\" in result:\n",
    "        candidates = result[\"issues\"]\n",
    "    elif isinstance(result, list):\n",
    "        candidates = result\n",
    "    else:\n",
    "        candidates = [result]\n",
    "\n",
    "    for item in candidates:\n",
    "        if \"source_type\" not in item:\n",
    "            item[\"source_type\"] = \"TEXTBOOK_NARRATIVE\"\n",
    "        if \"quotes\" in item:\n",
    "            for quote in item[\"quotes\"]:\n",
    "                if isinstance(quote, dict):\n",
    "                    page_offset = quote.get(\"page_offset\", 0)\n",
    "                    quote[\"page\"] = start_page + page_offset + 1\n",
    "\n",
    "    if debug:\n",
    "        print(f\"  [miner]: {len(candidates)} issue(s) found\")\n",
    "\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lhJqcFhj4S3u",
   "metadata": {
    "id": "lhJqcFhj4S3u"
   },
   "outputs": [],
   "source": [
    "def run_council(controversy: str, quotes: list, explanation: str, source_type: str = \"Unknown\", debug: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Runs council of models sequentially, then Meta-Jury synthesis using OpenAI.\n",
    "    \"\"\"\n",
    "    council_results = []\n",
    "\n",
    "    formatted_quotes = []\n",
    "    for q in quotes:\n",
    "        text = q.get('text', '') if isinstance(q, dict) else str(q)\n",
    "        formatted_quotes.append(f\"- {text}\")\n",
    "\n",
    "    prompt = COUNCIL_PROMPT.format(\n",
    "        controversy=controversy,\n",
    "        source_type=source_type,\n",
    "        quotes=\"\\n\".join(formatted_quotes),\n",
    "        explanation=explanation\n",
    "    )\n",
    "\n",
    "    MAX_RETRIES = 3\n",
    "\n",
    "    for model in COUNCIL_MODELS:\n",
    "        model_name = model.split('/')[-1]\n",
    "        if debug:\n",
    "            print(f\"  [council] querying {model_name}...\")\n",
    "\n",
    "        result = None\n",
    "        valid = False\n",
    "\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            result = analyze_text(prompt, \"\", model)\n",
    "\n",
    "            has_severity = \"severity\" in result\n",
    "            has_confidence = \"confidence\" in result\n",
    "            has_category = \"category\" in result\n",
    "            has_reasoning = \"reasoning\" in result and bool(result.get(\"reasoning\", \"\").strip())\n",
    "\n",
    "            severity_valid = False\n",
    "            if has_severity:\n",
    "                try:\n",
    "                    test_severity = float(result.get(\"severity\", -1))\n",
    "                    severity_valid = 1 <= test_severity <= 7\n",
    "                except Exception:\n",
    "                    severity_valid = False\n",
    "\n",
    "            confidence_valid = False\n",
    "            if has_confidence:\n",
    "                try:\n",
    "                    test_confidence = float(result.get(\"confidence\", -1))\n",
    "                    confidence_valid = 0 <= test_confidence <= 1\n",
    "                except Exception:\n",
    "                    confidence_valid = False\n",
    "\n",
    "            if has_severity and has_confidence and has_category and has_reasoning and severity_valid and confidence_valid:\n",
    "                valid = True\n",
    "                if debug and attempt > 0:\n",
    "                    print(f\"  [council] {model_name} passed on attempt {attempt + 1}\")\n",
    "                break\n",
    "\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                if debug:\n",
    "                    print(f\"  [council] {model_name} retry {attempt + 1}/{MAX_RETRIES} \"\n",
    "                          f\"(severity={severity_valid}, confidence={confidence_valid}, \"\n",
    "                          f\"category={has_category}, reasoning={has_reasoning})\")\n",
    "            else:\n",
    "                print(f\"  {model_name} dropped (failed validation after {MAX_RETRIES} attempts)\")\n",
    "\n",
    "        if not valid:\n",
    "            continue\n",
    "\n",
    "        attribution = result.get(\"attribution\", \"UNKNOWN\")\n",
    "        category = result.get(\"category\", \"Unknown\")\n",
    "        reasoning = result.get(\"reasoning\", \"\")\n",
    "\n",
    "        raw_severity = result.get(\"severity\", 4)\n",
    "        severity = 4\n",
    "        try:\n",
    "            if isinstance(raw_severity, (int, float)):\n",
    "                severity = float(raw_severity)\n",
    "            elif isinstance(raw_severity, str):\n",
    "                s = str(raw_severity).strip()\n",
    "                if \"-\" in s:\n",
    "                    parts = s.split(\"-\")\n",
    "                    nums = [float(p) for p in parts if p.strip().replace('.', '', 1).isdigit()]\n",
    "                    if len(nums) >= 2:\n",
    "                        severity = sum(nums[:2]) / 2\n",
    "                    elif len(nums) == 1:\n",
    "                        severity = nums[0]\n",
    "                else:\n",
    "                    match = re.search(r\"(\\d+(\\.\\d+)?)\", s)\n",
    "                    if match:\n",
    "                        severity = float(match.group(1))\n",
    "        except Exception:\n",
    "            severity = 4\n",
    "        severity = max(1, min(7, severity))\n",
    "\n",
    "        raw_confidence = result.get(\"confidence\", 0.5)\n",
    "        confidence = 0.5\n",
    "        try:\n",
    "            if isinstance(raw_confidence, (int, float)):\n",
    "                confidence = float(raw_confidence)\n",
    "            elif isinstance(raw_confidence, str):\n",
    "                s = str(raw_confidence).strip()\n",
    "                match = re.search(r\"(\\d+(\\.\\d+)?)\", s)\n",
    "                if match:\n",
    "                    confidence = float(match.group(1))\n",
    "        except Exception:\n",
    "            confidence = 0.5\n",
    "        confidence = max(0, min(1, confidence))\n",
    "\n",
    "        if debug:\n",
    "            print(f\"  [council]: {model_name}: {severity}/7, conf={confidence:.2f}, cat={category}\")\n",
    "\n",
    "        council_results.append({\n",
    "            \"model\": model_name,\n",
    "            \"attribution\": attribution,\n",
    "            \"category\": category,\n",
    "            \"severity\": severity,\n",
    "            \"confidence\": confidence,\n",
    "            \"reasoning\": reasoning\n",
    "        })\n",
    "\n",
    "    if not council_results:\n",
    "        print(f\"  All jurors dropped: returning fallback result\")\n",
    "        return {\n",
    "            \"final_severity\": 1,\n",
    "            \"system_confidence\": 0.0,\n",
    "            \"final_category\": \"Unknown\",\n",
    "            \"final_attribution\": \"UNKNOWN\",\n",
    "            \"flag_for_human_review\": True,\n",
    "            \"synthesis_reasoning\": \"All jurors failed validation. Manual review required.\",\n",
    "            \"variance_analysis\": \"No valid juror responses.\",\n",
    "            \"individual_jurors\": []\n",
    "        }\n",
    "\n",
    "    juror_evaluations = []\n",
    "    for idx, jr in enumerate(council_results, 1):\n",
    "        juror_evaluations.append(\n",
    "            f\"**Juror {idx} ({jr['model']}):**\\n\"\n",
    "            f\"- Category: {jr['category']}\\n\"\n",
    "            f\"- Severity: {jr['severity']}/7\\n\"\n",
    "            f\"- Confidence: {jr['confidence']:.2f}\\n\"\n",
    "            f\"- Attribution: {jr['attribution']}\\n\"\n",
    "            f\"- Reasoning: {jr['reasoning']}\\n\"\n",
    "        )\n",
    "\n",
    "    meta_prompt = META_JURY_PROMPT.format(\n",
    "        controversy=controversy,\n",
    "        source_type=source_type,\n",
    "        quotes=\"\\n\".join(formatted_quotes),\n",
    "        explanation=explanation,\n",
    "        juror_evaluations=\"\\n\".join(juror_evaluations)\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        print(f\"  [meta-jury] synthesizing {len(council_results)} juror(s) via {META_JURY_MODEL}...\")\n",
    "\n",
    "    try:\n",
    "        meta_response = openai_client.chat.completions.create(\n",
    "            model=META_JURY_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": meta_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"Synthesize the juror evaluations and provide your final verdict.\"}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "\n",
    "        meta_text = meta_response.choices[0].message.content.strip()\n",
    "        meta_result = json.loads(meta_text, strict=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Meta-Jury unavailable: {str(e)}\"\n",
    "        print(f\"  ✗ Meta-Jury failed: {str(e)}\")\n",
    "        return {\n",
    "            \"final_severity\": None,\n",
    "            \"system_confidence\": None,\n",
    "            \"final_category\": None,\n",
    "            \"final_attribution\": None,\n",
    "            \"flag_for_human_review\": True,\n",
    "            \"synthesis_reasoning\": error_msg,\n",
    "            \"variance_analysis\": \"Meta-Jury call failed. No verdict produced.\",\n",
    "            \"individual_jurors\": council_results\n",
    "        }\n",
    "\n",
    "    if debug:\n",
    "        flagged = meta_result.get('flag_for_human_review')\n",
    "        print(f\"  [meta-jury]: {meta_result.get('final_severity')}/7, \"\n",
    "              f\"conf={meta_result.get('system_confidence', 0):.2f}\"\n",
    "              + (\" flagged\" if flagged else \"\"))\n",
    "\n",
    "    return {\n",
    "        \"final_severity\": meta_result.get(\"final_severity\"),\n",
    "        \"system_confidence\": meta_result.get(\"system_confidence\"),\n",
    "        \"final_category\": meta_result.get(\"final_category\"),\n",
    "        \"final_attribution\": meta_result.get(\"final_attribution\"),\n",
    "        \"flag_for_human_review\": meta_result.get(\"flag_for_human_review\", False),\n",
    "        \"synthesis_reasoning\": meta_result.get(\"synthesis_summary\", meta_result.get(\"synthesis_reasoning\", \"\")),\n",
    "        \"variance_analysis\": meta_result.get(\"variance_note\", meta_result.get(\"variance_analysis\", \"\")),\n",
    "        \"individual_jurors\": council_results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6121b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4e6121b9",
    "outputId": "16ef7165-ffa2-417d-d8d8-f609182fcc85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chapter-based analysis of: ./Manuale Istorie/Clasa a 11 a CORVINUL.pdf\n",
      "Processing 5 pages per batch\n",
      "============================================================\n",
      "Total pages: 144\n",
      "\n",
      "\n",
      "============================================================\n",
      "BATCH 1: Pages 1-5\n",
      "============================================================\n",
      "Analyzing 5 pages together...\n",
      "\n",
      "[DEBUG] ========== STARTING MINER BATCH ==========\n",
      "[DEBUG] PDF: ./Manuale Istorie/Clasa a 11 a CORVINUL.pdf\n",
      "[DEBUG] Processing pages 1 to 5\n",
      "[DEBUG] ===========================================\n",
      "\n",
      "[DEBUG] Added page 1:\n",
      "[DEBUG]   - Base64 length: 1,075,396 chars\n",
      "[DEBUG] Added page 2:\n",
      "[DEBUG]   - Base64 length: 7,412 chars\n",
      "[DEBUG] Added page 3:\n",
      "[DEBUG]   - Base64 length: 124,264 chars\n",
      "[DEBUG] Added page 4:\n",
      "[DEBUG]   - Base64 length: 390,108 chars\n",
      "[DEBUG] Added page 5:\n",
      "[DEBUG]   - Base64 length: 717,664 chars\n",
      "[DEBUG] Average image size: 462,968 chars\n",
      "[DEBUG] System prompt length: 3300 chars\n",
      "[DEBUG] Expected JSON format: {\"issues\": [...]}\n",
      "[DEBUG] ====================================\n",
      "\n",
      "\n",
      "[DEBUG] ========== API RESPONSE ==========\n",
      "[DEBUG] Response text length: 1020 chars\n",
      "[DEBUG] Full raw response text:\n",
      "{\n",
      "  \"issues\": [\n",
      "    {\n",
      "      \"controversy\": \"Nationalistic Lyrics\",\n",
      "      \"source_type\": \"PRIMARY_SOURCE_USAGE\",\n",
      "      \"quotes\": [\n",
      "        {\n",
      "          \"text\": \"Deșteaptă-te, române!\",\n",
      "          \"page_offset\": 0\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"În care te-adânciră barbarii de tirani!\",\n",
      "          \"page_offset\": 0\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"Acum ori niciodată, croiește-ți altă soarte,\",\n",
      "          \"page_offset\": 0\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"La care să se-nchine și cruzi tăi dușmani!\",\n",
      "          \"page_offset\": 0\n",
      "        }\n",
      "      ],\n",
      "      \"explanation\": \"The poem 'Deșteaptă-te, române!' contains nationalistic and potentially divisive language, calling for resistance against 'barbarian tyrants' and referencing historical figures and events in a way that could be interpreted as promoting Romanian nationalism. While this is a historical primary source, its inclusion and presentation may influence students' interpretation of historical events through a nationalistic lens.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "[DEBUG] ====================================\n",
      "\n",
      "[DEBUG] Parsed JSON type: <class 'dict'>\n",
      "[DEBUG] Parsed JSON keys: ['issues']\n",
      "[DEBUG] Final count: 1 issues\n",
      "[DEBUG] ========== MINER BATCH COMPLETE ==========\n",
      "Found 1 potential issue(s)\n",
      "\n",
      "\n",
      "--- Issue 1: Nationalistic Lyrics ---\n",
      "[DEBUG] Calling council for evaluation...\n",
      "[DEBUG] Controversy: Nationalistic Lyrics\n",
      "[DEBUG] Quotes count: 4\n",
      "\n",
      "[DEBUG] ========== STARTING COUNCIL EVALUATION ==========\n",
      "[DEBUG] Model count: 5\n",
      "[DEBUG] Expected source_type: PRIMARY_SOURCE_USAGE\n",
      "  > Querying Mixtral-8x7B-Instruct-v0.1...\n",
      "[DEBUG] Mixtral-8x7B-Instruct-v0.1 returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ Mixtral-8x7B-Instruct-v0.1: Severity=3.0/7, Confidence=0.80, Category=National or Cultural Centering\n",
      "  > Querying gpt-oss-120b...\n",
      "[DEBUG] gpt-oss-120b returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ gpt-oss-120b: Severity=2.0/7, Confidence=0.85, Category=National or Cultural Centering\n",
      "  > Querying DeepSeek-V3.1...\n",
      "[DEBUG] DeepSeek-V3.1 validation failed (attempt 1/3), retrying...\n",
      "[DEBUG] Valid fields: severity=False, confidence=False, category=False, reasoning=False\n",
      "[DEBUG] DeepSeek-V3.1 succeeded on attempt 2\n",
      "[DEBUG] DeepSeek-V3.1 returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ DeepSeek-V3.1: Severity=2.0/7, Confidence=0.90, Category=Primary Source Framing\n",
      "  > Querying cogito-v2-1-671b...\n",
      "[DEBUG] cogito-v2-1-671b returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ cogito-v2-1-671b: Severity=2.0/7, Confidence=0.90, Category=Primary Source Framing\n",
      "  > Querying Kimi-K2-Thinking...\n",
      "[DEBUG] Kimi-K2-Thinking returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ Kimi-K2-Thinking: Severity=2.0/7, Confidence=0.80, Category=Primary Source Framing\n",
      "\n",
      "[DEBUG] ========== CALLING META-JURY ==========\n",
      "[DEBUG] Using model: Qwen/Qwen3.5-397B-A17B\n",
      "[DEBUG] Valid jurors in council: 5\n",
      "[DEBUG] Meta-Jury verdict: Severity=2/7, Confidence=0.87, Flagged=False\n",
      "[DEBUG] ========== COUNCIL COMPLETE ==========\n",
      "[DEBUG] Council evaluation complete: 2/7\n",
      "✓ Saved! Severity: 2/7, Category: Primary Source Framing, Confidence: 0.87\n",
      "\n",
      "============================================================\n",
      "BATCH 2: Pages 6-10\n",
      "============================================================\n",
      "Analyzing 5 pages together...\n",
      "\n",
      "[DEBUG] ========== STARTING MINER BATCH ==========\n",
      "[DEBUG] PDF: ./Manuale Istorie/Clasa a 11 a CORVINUL.pdf\n",
      "[DEBUG] Processing pages 6 to 10\n",
      "[DEBUG] ===========================================\n",
      "\n",
      "[DEBUG] Added page 6:\n",
      "[DEBUG]   - Base64 length: 412,180 chars\n",
      "[DEBUG] Added page 7:\n",
      "[DEBUG]   - Base64 length: 1,282,912 chars\n",
      "[DEBUG] Added page 8:\n",
      "[DEBUG]   - Base64 length: 1,250,748 chars\n",
      "[DEBUG] Added page 9:\n",
      "[DEBUG]   - Base64 length: 1,325,928 chars\n",
      "[DEBUG] Added page 10:\n",
      "[DEBUG]   - Base64 length: 1,214,724 chars\n",
      "[DEBUG] Average image size: 1,097,298 chars\n",
      "[DEBUG] System prompt length: 3300 chars\n",
      "[DEBUG] Expected JSON format: {\"issues\": [...]}\n",
      "[DEBUG] ====================================\n",
      "\n",
      "\n",
      "[DEBUG] ========== API RESPONSE ==========\n",
      "[DEBUG] Response text length: 2762 chars\n",
      "[DEBUG] Full raw response text:\n",
      "{\"issues\": [\n",
      "    {\n",
      "        \"controversy\": \"Historical interpretation influence\",\n",
      "        \"source_type\": \"TEXTBOOK_NARRATIVE\",\n",
      "        \"quotes\": [\n",
      "            {\"text\": \"Dacă primii ani ai secolului XX sunt dominați de o Europă model al civilizației universale, ce a urmat poate fi considerat, potrivit expresiei istoricului englez Eric Hobsbawn, „o epocă a catastrofei” (1914-1945) urmată de „o epocă de aur” (1945-1971/1973), căreia i-a succedat o epocă de decomunere și criză punctată în 1989 de prăbușirea regimurilor comuniste din Estul Europei.\", \"page_offset\": 0}\n",
      "        ],\n",
      "        \"explanation\": \"The passage uses Eric Hobsbawn's periodization of the 20th century, labeling 1914-1945 as 'an age of catastrophe' and 1945-1973 as 'a golden age'. This framing could influence students' interpretation by presenting a particular historical narrative as fact, potentially normalizing a Western-centric perspective on European history.\"\n",
      "    },\n",
      "    {\n",
      "        \"controversy\": \"Selective emphasis on historical events\",\n",
      "        \"source_type\": \"TEXTBOOK_NARRATIVE\",\n",
      "        \"quotes\": [\n",
      "            {\"text\": \"Evenimentele dramatice care au avut loc în acest secol al vieții au marcat existența popoarelor europene și a întregii omeniri ridicând serioase semne de întrebare privitoare la capacitatea omului de a-și asigura continuitatea vieții pe pământ și de a păstra în folosul său descoperirile senzaționale din domeniile științei și tehnicii și în general ale cunoașterii umane.\", \"page_offset\": 1}\n",
      "        ],\n",
      "        \"explanation\": \"The passage emphasizes dramatic events of the 20th century and their impact on European peoples, raising questions about humanity's ability to ensure its continuity. This selective focus on negative aspects could influence students by creating a predominantly negative view of the century's history.\"\n",
      "    },\n",
      "    {\n",
      "        \"controversy\": \"Presentation of historical sources\",\n",
      "        \"source_type\": \"PRIMARY_SOURCE_USAGE\",\n",
      "        \"quotes\": [\n",
      "            {\"text\": \"„Peisajul englez posedă o calitate pe care peisajele altor națiuni, inevitabil nu reușesc să o aibă. Aceaste este, cred eu, de o calitate care se face remarcată oricărui observator obiectiv iar ea exprimată de termenul mărime.[…] Lipsa unui loc aparte frumuseții pământului este ca și cum pământul își cunoaște propria frumusețe și mărime și nu simte nevoia să o strige”.\", \"page_offset\": 0}\n",
      "        ],\n",
      "        \"explanation\": \"The inclusion of Kazuo Ishiguro's quote about the English landscape having a unique quality could influence interpretation by presenting a subjective view as an objective observation. The lack of critical context around the quote may lead students to accept the described perspective without questioning its bias or universality.\"\n",
      "    }\n",
      "]}\n",
      "[DEBUG] ====================================\n",
      "\n",
      "[DEBUG] Parsed JSON type: <class 'dict'>\n",
      "[DEBUG] Parsed JSON keys: ['issues']\n",
      "[DEBUG] Final count: 3 issues\n",
      "[DEBUG] ========== MINER BATCH COMPLETE ==========\n",
      "Found 3 potential issue(s)\n",
      "\n",
      "\n",
      "--- Issue 1: Historical interpretation influence ---\n",
      "[DEBUG] Calling council for evaluation...\n",
      "[DEBUG] Controversy: Historical interpretation influence\n",
      "[DEBUG] Quotes count: 1\n",
      "\n",
      "[DEBUG] ========== STARTING COUNCIL EVALUATION ==========\n",
      "[DEBUG] Model count: 5\n",
      "[DEBUG] Expected source_type: TEXTBOOK_NARRATIVE\n",
      "  > Querying Mixtral-8x7B-Instruct-v0.1...\n",
      "[DEBUG] Mixtral-8x7B-Instruct-v0.1 returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ Mixtral-8x7B-Instruct-v0.1: Severity=3.0/7, Confidence=0.90, Category=Interpretation & Pedagogy\n",
      "  > Querying gpt-oss-120b...\n",
      "[DEBUG] gpt-oss-120b validation failed (attempt 1/3), retrying...\n",
      "[DEBUG] Valid fields: severity=False, confidence=False, category=False, reasoning=False\n",
      "[DEBUG] gpt-oss-120b succeeded on attempt 2\n",
      "[DEBUG] gpt-oss-120b returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ gpt-oss-120b: Severity=3.0/7, Confidence=0.78, Category=Narrative Framing\n",
      "  > Querying DeepSeek-V3.1...\n",
      "[DEBUG] DeepSeek-V3.1 returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ DeepSeek-V3.1: Severity=3.0/7, Confidence=0.80, Category=National or Cultural Centering\n",
      "  > Querying cogito-v2-1-671b...\n",
      "[DEBUG] cogito-v2-1-671b returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ cogito-v2-1-671b: Severity=2.0/7, Confidence=0.85, Category=National or Cultural Centering\n",
      "  > Querying Kimi-K2-Thinking...\n",
      "[DEBUG] Kimi-K2-Thinking returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ Kimi-K2-Thinking: Severity=4.0/7, Confidence=0.90, Category=National or Cultural Centering\n",
      "\n",
      "[DEBUG] ========== CALLING META-JURY ==========\n",
      "[DEBUG] Using model: Qwen/Qwen3.5-397B-A17B\n",
      "[DEBUG] Valid jurors in council: 5\n",
      "[DEBUG] Meta-Jury verdict: Severity=3/7, Confidence=0.82, Flagged=True\n",
      "[DEBUG] ========== COUNCIL COMPLETE ==========\n",
      "[DEBUG] Council evaluation complete: 3/7\n",
      "✓ Saved! Severity: 3/7, Category: National or Cultural Centering, Confidence: 0.82 ⚠️  [FLAGGED FOR REVIEW]\n",
      "\n",
      "--- Issue 2: Selective emphasis on historical events ---\n",
      "[DEBUG] Calling council for evaluation...\n",
      "[DEBUG] Controversy: Selective emphasis on historical events\n",
      "[DEBUG] Quotes count: 1\n",
      "\n",
      "[DEBUG] ========== STARTING COUNCIL EVALUATION ==========\n",
      "[DEBUG] Model count: 5\n",
      "[DEBUG] Expected source_type: TEXTBOOK_NARRATIVE\n",
      "  > Querying Mixtral-8x7B-Instruct-v0.1...\n",
      "[DEBUG] Mixtral-8x7B-Instruct-v0.1 returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ Mixtral-8x7B-Instruct-v0.1: Severity=3.0/7, Confidence=0.90, Category=Structure & Emphasis\n",
      "  > Querying gpt-oss-120b...\n",
      "[DEBUG] gpt-oss-120b returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ gpt-oss-120b: Severity=3.0/7, Confidence=0.86, Category=Selection Bias\n",
      "  > Querying DeepSeek-V3.1...\n",
      "[DEBUG] DeepSeek-V3.1 returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ DeepSeek-V3.1: Severity=2.0/7, Confidence=0.80, Category=Selection Bias\n",
      "  > Querying cogito-v2-1-671b...\n",
      "[DEBUG] cogito-v2-1-671b returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ cogito-v2-1-671b: Severity=2.0/7, Confidence=0.85, Category=Asymmetrical Detail\n",
      "  > Querying Kimi-K2-Thinking...\n",
      "[DEBUG] Kimi-K2-Thinking returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ Kimi-K2-Thinking: Severity=4.0/7, Confidence=0.85, Category=Selection Bias\n",
      "\n",
      "[DEBUG] ========== CALLING META-JURY ==========\n",
      "[DEBUG] Using model: Qwen/Qwen3.5-397B-A17B\n",
      "[DEBUG] Valid jurors in council: 5\n",
      "  ✗ Meta-Jury failed: Expecting value: line 1 column 1 (char 0)\n",
      "[DEBUG] Council evaluation complete: None/7\n",
      "✓ Saved! Severity: N/A (Meta-Jury failed), Category: N/A, Confidence: N/A ⚠️  [FLAGGED FOR REVIEW]\n",
      "\n",
      "--- Issue 3: Presentation of historical sources ---\n",
      "[DEBUG] Calling council for evaluation...\n",
      "[DEBUG] Controversy: Presentation of historical sources\n",
      "[DEBUG] Quotes count: 1\n",
      "\n",
      "[DEBUG] ========== STARTING COUNCIL EVALUATION ==========\n",
      "[DEBUG] Model count: 5\n",
      "[DEBUG] Expected source_type: PRIMARY_SOURCE_USAGE\n",
      "  > Querying Mixtral-8x7B-Instruct-v0.1...\n",
      "[DEBUG] Mixtral-8x7B-Instruct-v0.1 returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ Mixtral-8x7B-Instruct-v0.1: Severity=3.0/7, Confidence=0.80, Category=Primary Source Framing\n",
      "  > Querying gpt-oss-120b...\n",
      "[DEBUG] gpt-oss-120b validation failed (attempt 1/3), retrying...\n",
      "[DEBUG] Valid fields: severity=False, confidence=False, category=False, reasoning=False\n",
      "[DEBUG] gpt-oss-120b succeeded on attempt 2\n",
      "[DEBUG] gpt-oss-120b returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ gpt-oss-120b: Severity=3.0/7, Confidence=0.90, Category=Primary Source Framing\n",
      "  > Querying DeepSeek-V3.1...\n",
      "[DEBUG] DeepSeek-V3.1 returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ DeepSeek-V3.1: Severity=2.0/7, Confidence=0.90, Category=Primary Source Framing\n",
      "  > Querying cogito-v2-1-671b...\n",
      "[DEBUG] cogito-v2-1-671b returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ cogito-v2-1-671b: Severity=2.0/7, Confidence=0.90, Category=Primary Source Framing\n",
      "  > Querying Kimi-K2-Thinking...\n",
      "[DEBUG] Kimi-K2-Thinking returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ Kimi-K2-Thinking: Severity=3.0/7, Confidence=0.85, Category=Primary Source Framing\n",
      "\n",
      "[DEBUG] ========== CALLING META-JURY ==========\n",
      "[DEBUG] Using model: Qwen/Qwen3.5-397B-A17B\n",
      "[DEBUG] Valid jurors in council: 5\n",
      "[DEBUG] Meta-Jury verdict: Severity=3/7, Confidence=0.88, Flagged=False\n",
      "[DEBUG] ========== COUNCIL COMPLETE ==========\n",
      "[DEBUG] Council evaluation complete: 3/7\n",
      "✓ Saved! Severity: 3/7, Category: Primary Source Framing, Confidence: 0.88\n",
      "\n",
      "============================================================\n",
      "BATCH 3: Pages 11-15\n",
      "============================================================\n",
      "Analyzing 5 pages together...\n",
      "\n",
      "[DEBUG] ========== STARTING MINER BATCH ==========\n",
      "[DEBUG] PDF: ./Manuale Istorie/Clasa a 11 a CORVINUL.pdf\n",
      "[DEBUG] Processing pages 11 to 15\n",
      "[DEBUG] ===========================================\n",
      "\n",
      "[DEBUG] Added page 11:\n",
      "[DEBUG]   - Base64 length: 1,317,404 chars\n",
      "[DEBUG] Added page 12:\n",
      "[DEBUG]   - Base64 length: 1,226,176 chars\n",
      "[DEBUG] Added page 13:\n",
      "[DEBUG]   - Base64 length: 1,397,256 chars\n",
      "[DEBUG] Added page 14:\n",
      "[DEBUG]   - Base64 length: 1,204,252 chars\n",
      "[DEBUG] Added page 15:\n",
      "[DEBUG]   - Base64 length: 1,277,808 chars\n",
      "[DEBUG] Average image size: 1,284,579 chars\n",
      "[DEBUG] System prompt length: 3300 chars\n",
      "[DEBUG] Expected JSON format: {\"issues\": [...]}\n",
      "[DEBUG] ====================================\n",
      "\n",
      "\n",
      "[DEBUG] ========== API RESPONSE ==========\n",
      "[DEBUG] Response text length: 2858 chars\n",
      "[DEBUG] Full raw response text:\n",
      "{\n",
      "  \"issues\": [\n",
      "    {\n",
      "      \"controversy\": \"Potential bias in historical interpretation\",\n",
      "      \"source_type\": \"TEXTBOOK_NARRATIVE\",\n",
      "      \"quotes\": [\n",
      "        {\n",
      "          \"text\": \"Secolul al XX-lea rămâne în istoria umanității ca o perioadă în care au avut loc două mari catastrofe, două mari conflagrații mondiale.\",\n",
      "          \"page_offset\": 0\n",
      "        }\n",
      "      ],\n",
      "      \"explanation\": \"The passage could influence interpretation by framing the 20th century solely through the lens of major catastrophes and wars, potentially overshadowing other significant historical developments.\"\n",
      "    },\n",
      "    {\n",
      "      \"controversy\": \"Value-laden description of historical events\",\n",
      "      \"source_type\": \"TEXTBOOK_NARRATIVE\",\n",
      "      \"quotes\": [\n",
      "        {\n",
      "          \"text\": \"Spre finele secolului al XX-lea popoarele din Estul Europei se vor angaja și ele pe calea integrării euroatlantice.\",\n",
      "          \"page_offset\": 0\n",
      "        }\n",
      "      ],\n",
      "      \"explanation\": \"The use of 'se vor angaja' (will engage) implies a positive direction or aspiration towards Euro-Atlantic integration, potentially influencing students' perception of its desirability.\"\n",
      "    },\n",
      "    {\n",
      "      \"controversy\": \"Selective emphasis on certain historical aspects\",\n",
      "      \"source_type\": \"TEXTBOOK_NARRATIVE\",\n",
      "      \"quotes\": [\n",
      "        {\n",
      "          \"text\": \"Unitatea Europei în secolul al XX-lea, un subiect complex și controversat.\",\n",
      "          \"page_offset\": 1\n",
      "        }\n",
      "      ],\n",
      "      \"explanation\": \"By labeling European unity as 'complex and controversial,' the textbook may be influencing students to view it with a particular critical perspective.\"\n",
      "    },\n",
      "    {\n",
      "      \"controversy\": \"Potential bias in presenting historical figures\",\n",
      "      \"source_type\": \"TEXTBOOK_NARRATIVE\",\n",
      "      \"quotes\": [\n",
      "        {\n",
      "          \"text\": \"Născut la Praga în anul 1936, Vaçlav Havel s-a remarcat ca scriitor și om politic. A câştigat apreciere internațională în anii '60 ai secolului al XX-lea pentru opere ca Petrecerea din grădină și Memorandum care sunt parabole umoristice ale vieții din comunism.\",\n",
      "          \"page_offset\": 0\n",
      "        }\n",
      "      ],\n",
      "      \"explanation\": \"The description of Vaclav Havel focuses on his positive attributes and international recognition, potentially presenting a biased or overly positive portrayal of the figure.\"\n",
      "    },\n",
      "    {\n",
      "      \"controversy\": \"Framing of European identity and values\",\n",
      "      \"source_type\": \"TEXTBOOK_NARRATIVE\",\n",
      "      \"quotes\": [\n",
      "        {\n",
      "          \"text\": \"Civilizația europeană are la temelia valorilor creștine aflate mereu într-un proces de transformare și de adaptare la realitățile vremii.\",\n",
      "          \"page_offset\": 2\n",
      "        }\n",
      "      ],\n",
      "      \"explanation\": \"The emphasis on Christian values as foundational to European civilization may influence students' understanding of European identity and potentially marginalize non-Christian perspectives.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "[DEBUG] ====================================\n",
      "\n",
      "[DEBUG] Parsed JSON type: <class 'dict'>\n",
      "[DEBUG] Parsed JSON keys: ['issues']\n",
      "[DEBUG] Final count: 5 issues\n",
      "[DEBUG] ========== MINER BATCH COMPLETE ==========\n",
      "Found 5 potential issue(s)\n",
      "\n",
      "\n",
      "--- Issue 1: Potential bias in historical interpretation ---\n",
      "[DEBUG] Calling council for evaluation...\n",
      "[DEBUG] Controversy: Potential bias in historical interpretation\n",
      "[DEBUG] Quotes count: 1\n",
      "\n",
      "[DEBUG] ========== STARTING COUNCIL EVALUATION ==========\n",
      "[DEBUG] Model count: 5\n",
      "[DEBUG] Expected source_type: TEXTBOOK_NARRATIVE\n",
      "  > Querying Mixtral-8x7B-Instruct-v0.1...\n",
      "[DEBUG] Mixtral-8x7B-Instruct-v0.1 returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ Mixtral-8x7B-Instruct-v0.1: Severity=3.0/7, Confidence=0.80, Category=Narrative Framing\n",
      "  > Querying gpt-oss-120b...\n",
      "[DEBUG] gpt-oss-120b returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ gpt-oss-120b: Severity=3.0/7, Confidence=0.92, Category=Narrative Framing\n",
      "  > Querying DeepSeek-V3.1...\n",
      "[DEBUG] DeepSeek-V3.1 returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ DeepSeek-V3.1: Severity=2.0/7, Confidence=0.90, Category=Narrative Framing\n",
      "  > Querying cogito-v2-1-671b...\n",
      "[DEBUG] cogito-v2-1-671b returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ cogito-v2-1-671b: Severity=3.0/7, Confidence=0.80, Category=Omission / Underdevelopment\n",
      "  > Querying Kimi-K2-Thinking...\n",
      "[DEBUG] Kimi-K2-Thinking validation failed (attempt 1/3), retrying...\n",
      "[DEBUG] Valid fields: severity=False, confidence=False, category=False, reasoning=False\n",
      "[DEBUG] Kimi-K2-Thinking succeeded on attempt 2\n",
      "[DEBUG] Kimi-K2-Thinking returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ Kimi-K2-Thinking: Severity=3.0/7, Confidence=0.85, Category=Narrative Framing\n",
      "\n",
      "[DEBUG] ========== CALLING META-JURY ==========\n",
      "[DEBUG] Using model: Qwen/Qwen3.5-397B-A17B\n",
      "[DEBUG] Valid jurors in council: 5\n",
      "[DEBUG] Meta-Jury verdict: Severity=3/7, Confidence=0.87, Flagged=False\n",
      "[DEBUG] ========== COUNCIL COMPLETE ==========\n",
      "[DEBUG] Council evaluation complete: 3/7\n",
      "✓ Saved! Severity: 3/7, Category: Narrative Framing, Confidence: 0.87\n",
      "\n",
      "--- Issue 2: Value-laden description of historical events ---\n",
      "[DEBUG] Calling council for evaluation...\n",
      "[DEBUG] Controversy: Value-laden description of historical events\n",
      "[DEBUG] Quotes count: 1\n",
      "\n",
      "[DEBUG] ========== STARTING COUNCIL EVALUATION ==========\n",
      "[DEBUG] Model count: 5\n",
      "[DEBUG] Expected source_type: TEXTBOOK_NARRATIVE\n",
      "  > Querying Mixtral-8x7B-Instruct-v0.1...\n",
      "[DEBUG] Mixtral-8x7B-Instruct-v0.1 returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ Mixtral-8x7B-Instruct-v0.1: Severity=3.0/7, Confidence=0.80, Category=Narrative Framing\n",
      "  > Querying gpt-oss-120b...\n",
      "[DEBUG] gpt-oss-120b returned keys: ['attribution', 'category', 'severity', 'confidence', 'reasoning', '_raw_response']\n",
      "  ✓ gpt-oss-120b: Severity=2.0/7, Confidence=0.88, Category=Narrative Framing\n",
      "  > Querying DeepSeek-V3.1...\n"
     ]
    }
   ],
   "source": [
    "def run_pipeline(pdf_path: str, pages_per_batch: int = 10, debug: bool = False):\n",
    "    \"\"\"Main execution loop: Process PDF in batches → Council evaluation.\"\"\"\n",
    "    print(f\"Starting analysis: {pdf_path}\")\n",
    "    print(f\"Batch size: {pages_per_batch} pages\")\n",
    "\n",
    "    total_pages = get_total_pages(pdf_path)\n",
    "    print(f\"Total pages: {total_pages}\\n\")\n",
    "\n",
    "    total_controversies = 0\n",
    "    batch_num = 0\n",
    "\n",
    "    for start_page in range(0, total_pages, pages_per_batch):\n",
    "        batch_num += 1\n",
    "        end_page = min(start_page + pages_per_batch, total_pages)\n",
    "        actual_pages = end_page - start_page\n",
    "\n",
    "        print(f\"\\nBatch {batch_num}  [pages {start_page+1}–{end_page}]\")\n",
    "\n",
    "        controversies = run_miner_batch(pdf_path, start_page, actual_pages, debug=debug)\n",
    "\n",
    "        if not controversies:\n",
    "            print(f\"  No issues found\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  Found {len(controversies)} potential issue(s)\")\n",
    "\n",
    "        for idx, item in enumerate(controversies, 1):\n",
    "            controversy = item.get(\"controversy\", \"Untitled\")\n",
    "            quotes = item.get(\"quotes\", [])\n",
    "            explanation = item.get(\"explanation\", \"\")\n",
    "            source_type = item.get(\"source_type\", \"TEXTBOOK_NARRATIVE\")\n",
    "\n",
    "            print(f\"\\n  Issue {idx}/{len(controversies)}: {controversy}\")\n",
    "\n",
    "            council_result = run_council(controversy, quotes, explanation, source_type, debug=debug)\n",
    "            final_severity = council_result[\"final_severity\"]\n",
    "\n",
    "            result = {\n",
    "                \"batch\": batch_num,\n",
    "                \"pages\": f\"{start_page+1}-{end_page}\",\n",
    "                \"controversy\": controversy,\n",
    "                \"quotes\": quotes,\n",
    "                \"explanation\": explanation,\n",
    "                \"source_type\": source_type,\n",
    "                \"final_severity\": final_severity,\n",
    "                \"system_confidence\": council_result[\"system_confidence\"],\n",
    "                \"final_category\": council_result[\"final_category\"],\n",
    "                \"final_attribution\": council_result[\"final_attribution\"],\n",
    "                \"flagged_for_review\": council_result[\"flag_for_human_review\"],\n",
    "                \"synthesis_reasoning\": council_result[\"synthesis_reasoning\"],\n",
    "                \"variance_analysis\": council_result.get(\"variance_analysis\", \"\"),\n",
    "                \"individual_jurors\": council_result[\"individual_jurors\"]\n",
    "            }\n",
    "\n",
    "            with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "                f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "\n",
    "            flagged = council_result[\"flag_for_human_review\"]\n",
    "            confidence = council_result[\"system_confidence\"]\n",
    "            category = council_result[\"final_category\"]\n",
    "\n",
    "            severity_str = f\"{final_severity}/7\" if final_severity is not None else \"N/A\"\n",
    "            confidence_str = f\"{confidence:.2f}\" if confidence is not None else \"N/A\"\n",
    "            category_str = category if category is not None else \"N/A\"\n",
    "            flag_str = \" FLAGGED\" if flagged else \"\"\n",
    "\n",
    "            print(f\"  Severity: {severity_str}  Category: {category_str}  Conf: {confidence_str}{flag_str}\")\n",
    "\n",
    "            total_controversies += 1\n",
    "\n",
    "    print(f\"Total: {total_controversies} controversies. Saved to {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "run_pipeline(\"./Manuale Istorie/Clasa a 11 a CORVINUL.pdf\", pages_per_batch=5, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453b6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report generated: controversy_report.html\n",
      "94 findings analyzed\n",
      "Average severity: 2.95/7\n",
      "High severity (>=5): 3\n",
      "Flagged for review: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'controversy_report.html'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_html_report(jsonl_file: str, output_html: str = \"controversy_report.html\"):\n",
    "    \"\"\"Generate a clean, readable HTML report from results.jsonl.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            results.append(json.loads(line, strict=False))\n",
    "    \n",
    "    results.sort(key=lambda x: x.get('final_severity', 0), reverse=True)\n",
    "    \n",
    "    all_categories = sorted(set(r.get('final_category', 'Unknown') for r in results))\n",
    "\n",
    "    total_severity = sum(r.get('final_severity', 0) for r in results)\n",
    "    avg_severity = total_severity / len(results) if results else 0\n",
    "    high_severity_count = sum(1 for r in results if r.get('final_severity', 0) >= 5)\n",
    "    flagged_count = sum(1 for r in results if r.get('flagged_for_review', False))\n",
    "    \n",
    "    html = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Textbook Analysis Report</title>\n",
    "    <style>\n",
    "        * {{\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "            box-sizing: border-box;\n",
    "        }}\n",
    "        \n",
    "        body {{\n",
    "            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n",
    "            line-height: 1.6;\n",
    "            color: #1a1a1a;\n",
    "            background-color: #f8f9fa;\n",
    "            padding: 40px 20px;\n",
    "        }}\n",
    "        \n",
    "        .container {{\n",
    "            max-width: 1000px;\n",
    "            margin: 0 auto;\n",
    "        }}\n",
    "        \n",
    "        h1 {{\n",
    "            color: #2c3e50;\n",
    "            font-size: 2em;\n",
    "            font-weight: 600;\n",
    "            margin-bottom: 30px;\n",
    "            letter-spacing: -0.5px;\n",
    "        }}\n",
    "        \n",
    "        .summary {{\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            margin-bottom: 20px;\n",
    "            border-left: 4px solid #2c3e50;\n",
    "            box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        \n",
    "        .summary p {{\n",
    "            margin: 8px 0;\n",
    "            color: #555;\n",
    "        }}\n",
    "        \n",
    "        .summary strong {{\n",
    "            color: #2c3e50;\n",
    "        }}\n",
    "\n",
    "        /* ── Category Filter Bar ── */\n",
    "        .filter-bar {{\n",
    "            background: white;\n",
    "            padding: 16px 20px;\n",
    "            margin-bottom: 24px;\n",
    "            box-shadow: 0 1px 3px rgba(0,0,0,0.08);\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "            gap: 12px;\n",
    "            flex-wrap: wrap;\n",
    "        }}\n",
    "\n",
    "        .filter-bar label {{\n",
    "            font-weight: 600;\n",
    "            color: #2c3e50;\n",
    "            font-size: 0.9em;\n",
    "            white-space: nowrap;\n",
    "        }}\n",
    "\n",
    "        .filter-btn {{\n",
    "            padding: 5px 14px;\n",
    "            border: 1px solid #d0d0d0;\n",
    "            border-radius: 20px;\n",
    "            background: #f8f9fa;\n",
    "            color: #444;\n",
    "            font-size: 0.85em;\n",
    "            cursor: pointer;\n",
    "            transition: all 0.15s;\n",
    "            white-space: nowrap;\n",
    "        }}\n",
    "\n",
    "        .filter-btn:hover {{\n",
    "            border-color: #2c3e50;\n",
    "            color: #2c3e50;\n",
    "        }}\n",
    "\n",
    "        .filter-btn.active {{\n",
    "            background: #2c3e50;\n",
    "            border-color: #2c3e50;\n",
    "            color: white;\n",
    "        }}\n",
    "\n",
    "        .filter-count {{\n",
    "            margin-left: auto;\n",
    "            font-size: 0.85em;\n",
    "            color: #888;\n",
    "        }}\n",
    "\n",
    "        .finding {{\n",
    "            background: white;\n",
    "            margin-bottom: 1px;\n",
    "            box-shadow: 0 1px 3px rgba(0,0,0,0.05);\n",
    "            transition: opacity 0.2s;\n",
    "        }}\n",
    "\n",
    "        .finding.hidden {{\n",
    "            display: none;\n",
    "        }}\n",
    "        \n",
    "        .finding-header {{\n",
    "            padding: 20px;\n",
    "            cursor: pointer;\n",
    "            display: flex;\n",
    "            justify-content: space-between;\n",
    "            align-items: center;\n",
    "            user-select: none;\n",
    "            transition: background-color 0.2s;\n",
    "        }}\n",
    "        \n",
    "        .finding-header:hover {{\n",
    "            background-color: #f8f9fa;\n",
    "        }}\n",
    "        \n",
    "        .finding-title {{\n",
    "            color: #2c3e50;\n",
    "            font-weight: 500;\n",
    "            flex: 1;\n",
    "            margin-right: 20px;\n",
    "        }}\n",
    "        \n",
    "        .finding-meta {{\n",
    "            display: flex;\n",
    "            gap: 15px;\n",
    "            align-items: center;\n",
    "        }}\n",
    "        \n",
    "        .finding-score {{\n",
    "            color: #2c3e50;\n",
    "            font-weight: 600;\n",
    "            font-size: 1.1em;\n",
    "        }}\n",
    "        \n",
    "        .finding-category {{\n",
    "            color: #666;\n",
    "            font-size: 0.9em;\n",
    "            background: #f0f0f0;\n",
    "            padding: 4px 8px;\n",
    "            border-radius: 3px;\n",
    "        }}\n",
    "        \n",
    "        .flagged-indicator {{\n",
    "            color: #e74c3c;\n",
    "            font-size: 0.9em;\n",
    "        }}\n",
    "        \n",
    "        .finding-content {{\n",
    "            display: none;\n",
    "            padding: 0 20px 20px 20px;\n",
    "            border-top: 1px solid #f0f0f0;\n",
    "        }}\n",
    "        \n",
    "        .finding.expanded .finding-content {{\n",
    "            display: block;\n",
    "        }}\n",
    "        \n",
    "        .metadata {{\n",
    "            color: #666;\n",
    "            font-size: 0.9em;\n",
    "            margin-bottom: 20px;\n",
    "            padding: 10px;\n",
    "            background: #f8f9fa;\n",
    "            border-radius: 3px;\n",
    "        }}\n",
    "        \n",
    "        .section {{\n",
    "            margin: 20px 0;\n",
    "        }}\n",
    "        \n",
    "        .section-title {{\n",
    "            color: #2c3e50;\n",
    "            font-weight: 600;\n",
    "            margin-bottom: 10px;\n",
    "            font-size: 1em;\n",
    "            text-transform: uppercase;\n",
    "            letter-spacing: 0.5px;\n",
    "        }}\n",
    "        \n",
    "        .explanation {{\n",
    "            background: #f8f9fa;\n",
    "            padding: 15px;\n",
    "            margin: 10px 0;\n",
    "            line-height: 1.7;\n",
    "            border-left: 3px solid #3498db;\n",
    "        }}\n",
    "        \n",
    "        .quote {{\n",
    "            background: #fff9e6;\n",
    "            padding: 12px;\n",
    "            margin: 8px 0;\n",
    "            border-left: 3px solid #f39c12;\n",
    "        }}\n",
    "        \n",
    "        .quote-text {{\n",
    "            color: #1a1a1a;\n",
    "            margin-bottom: 5px;\n",
    "            font-style: italic;\n",
    "        }}\n",
    "        \n",
    "        .quote-page {{\n",
    "            color: #999;\n",
    "            font-size: 0.85em;\n",
    "        }}\n",
    "        \n",
    "        .meta-jury {{\n",
    "            background: #e8f5e9;\n",
    "            padding: 15px;\n",
    "            margin: 15px 0;\n",
    "            border-left: 3px solid #4caf50;\n",
    "        }}\n",
    "        \n",
    "        .meta-jury-title {{\n",
    "            font-weight: 600;\n",
    "            color: #2e7d32;\n",
    "            margin-bottom: 10px;\n",
    "        }}\n",
    "        \n",
    "        .meta-jury-stats {{\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
    "            gap: 10px;\n",
    "            margin: 10px 0;\n",
    "        }}\n",
    "        \n",
    "        .meta-stat {{\n",
    "            background: white;\n",
    "            padding: 10px;\n",
    "            border-radius: 3px;\n",
    "        }}\n",
    "        \n",
    "        .meta-stat-label {{\n",
    "            font-size: 0.85em;\n",
    "            color: #666;\n",
    "        }}\n",
    "        \n",
    "        .meta-stat-value {{\n",
    "            font-size: 1.2em;\n",
    "            font-weight: 600;\n",
    "            color: #2e7d32;\n",
    "        }}\n",
    "        \n",
    "        table {{\n",
    "            width: 100%;\n",
    "            border-collapse: collapse;\n",
    "            margin-top: 10px;\n",
    "        }}\n",
    "        \n",
    "        th, td {{\n",
    "            padding: 12px;\n",
    "            text-align: left;\n",
    "            border-bottom: 1px solid #f0f0f0;\n",
    "            font-size: 0.9em;\n",
    "        }}\n",
    "        \n",
    "        th {{\n",
    "            background: #2c3e50;\n",
    "            color: white;\n",
    "            font-weight: 500;\n",
    "        }}\n",
    "        \n",
    "        .model-name {{\n",
    "            color: #2c3e50;\n",
    "            font-weight: 500;\n",
    "        }}\n",
    "        \n",
    "        .severity-badge {{\n",
    "            display: inline-block;\n",
    "            padding: 3px 8px;\n",
    "            border-radius: 3px;\n",
    "            font-weight: 600;\n",
    "            font-size: 0.9em;\n",
    "        }}\n",
    "        \n",
    "        .severity-1, .severity-2 {{\n",
    "            background: #d4edda;\n",
    "            color: #155724;\n",
    "        }}\n",
    "        \n",
    "        .severity-3, .severity-4 {{\n",
    "            background: #fff3cd;\n",
    "            color: #856404;\n",
    "        }}\n",
    "        \n",
    "        .severity-5, .severity-6 {{\n",
    "            background: #f8d7da;\n",
    "            color: #721c24;\n",
    "        }}\n",
    "        \n",
    "        .severity-7 {{\n",
    "            background: #721c24;\n",
    "            color: white;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Textbook Analysis Report — Multi-Dimensional Audit</h1>\n",
    "        \n",
    "        <div class=\"summary\">\n",
    "            <p><strong>Total findings:</strong> {len(results)}</p>\n",
    "            <p><strong>Average severity:</strong> {avg_severity:.2f}/7 (7-point Likert scale)</p>\n",
    "            <p><strong>High severity findings (≥5):</strong> {high_severity_count}</p>\n",
    "            <p><strong>Flagged for human review:</strong> {flagged_count}</p>\n",
    "            <p><strong>Evaluation method:</strong> Multi-dimensional audit with Meta-Jury synthesis</p>\n",
    "        </div>\n",
    "\n",
    "        <!-- Category Filter Bar -->\n",
    "        <div class=\"filter-bar\">\n",
    "            <label>Filter by category:</label>\n",
    "            <button class=\"filter-btn active\" data-filter=\"all\" onclick=\"filterFindings(this)\">All</button>\n",
    "\"\"\"\n",
    "    for cat in all_categories:\n",
    "        cat_js = cat.replace(\"'\", \"\\\\'\")\n",
    "        html += f'            <button class=\"filter-btn\" data-filter=\"{cat_js}\" onclick=\"filterFindings(this)\">{cat}</button>\\n'\n",
    "\n",
    "    html += f\"\"\"            <span class=\"filter-count\" id=\"filter-count\">{len(results)} of {len(results)} shown</span>\n",
    "        </div>\n",
    "\"\"\"\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        severity = result.get('final_severity', 0)\n",
    "        confidence = result.get('system_confidence', 0)\n",
    "        category = result.get('final_category', 'Unknown')\n",
    "        flagged = result.get('flagged_for_review', False)\n",
    "        flag_icon = ' ⚠️' if flagged else ''\n",
    "        \n",
    "        severity_class = f\"severity-{int(severity)}\"\n",
    "        cat_attr = category.replace('\"', '&quot;')\n",
    "        \n",
    "        html += f\"\"\"\n",
    "        <div class=\"finding\" data-category=\"{cat_attr}\" onclick=\"this.classList.toggle('expanded')\">\n",
    "            <div class=\"finding-header\">\n",
    "                <div class=\"finding-title\">{result['controversy']}{flag_icon}</div>\n",
    "                <div class=\"finding-meta\">\n",
    "                    <div class=\"finding-category\">{category}</div>\n",
    "                    <div class=\"finding-score\"><span class=\"severity-badge {severity_class}\">{severity}/7</span></div>\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"finding-content\">\n",
    "                <div class=\"metadata\">\n",
    "                    Pages {result['pages']} | Batch {result['batch']} | System Confidence: {confidence:.2f}\n",
    "                    {' | <span style=\"color: #e74c3c; font-weight: 500;\">⚠️ Flagged for Review</span>' if flagged else ''}\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"section\">\n",
    "                    <div class=\"section-title\">Miner's Analysis</div>\n",
    "                    <div class=\"explanation\">{result['explanation']}</div>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"section\">\n",
    "                    <div class=\"section-title\">Evidence</div>\n",
    "\"\"\"\n",
    "        \n",
    "        for quote in result['quotes']:\n",
    "            if isinstance(quote, dict):\n",
    "                quote_text = quote.get('text', '')\n",
    "                page_num = quote.get('page', quote.get('page_offset', 'Unknown'))\n",
    "            else:\n",
    "                quote_text = quote\n",
    "                page_num = 'Unknown'\n",
    "            \n",
    "            html += f\"\"\"\n",
    "                    <div class=\"quote\">\n",
    "                        <div class=\"quote-text\">\"{quote_text}\"</div>\n",
    "                        <div class=\"quote-page\">Page: {page_num}</div>\n",
    "                    </div>\n",
    "\"\"\"\n",
    "        \n",
    "        synthesis = result.get('synthesis_reasoning', 'N/A')\n",
    "        variance = result.get('variance_analysis', '')\n",
    "        \n",
    "        html += f\"\"\"\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"section\">\n",
    "                    <div class=\"meta-jury\">\n",
    "                        <div class=\"meta-jury-title\">🏛️ Meta-Jury Final Verdict</div>\n",
    "                        <div class=\"meta-jury-stats\">\n",
    "                            <div class=\"meta-stat\">\n",
    "                                <div class=\"meta-stat-label\">Final Severity</div>\n",
    "                                <div class=\"meta-stat-value\">{severity}/7</div>\n",
    "                            </div>\n",
    "                            <div class=\"meta-stat\">\n",
    "                                <div class=\"meta-stat-label\">System Confidence</div>\n",
    "                                <div class=\"meta-stat-value\">{confidence:.2f}</div>\n",
    "                            </div>\n",
    "                            <div class=\"meta-stat\">\n",
    "                                <div class=\"meta-stat-label\">Category</div>\n",
    "                                <div class=\"meta-stat-value\" style=\"font-size: 1em;\">{category}</div>\n",
    "                            </div>\n",
    "                        </div>\n",
    "                        <p style=\"margin-top: 10px;\"><strong>Synthesis:</strong> {synthesis}</p>\n",
    "                        {f'<p style=\"margin-top: 10px; color: #e74c3c;\"><strong>Variance Analysis:</strong> {variance}</p>' if variance else ''}\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"section\">\n",
    "                    <div class=\"section-title\">Individual Juror Evaluations</div>\n",
    "                    <table>\n",
    "                        <thead>\n",
    "                            <tr>\n",
    "                                <th>Juror Model</th>\n",
    "                                <th>Category</th>\n",
    "                                <th>Severity</th>\n",
    "                                <th>Confidence</th>\n",
    "                                <th>Reasoning</th>\n",
    "                            </tr>\n",
    "                        </thead>\n",
    "                        <tbody>\n",
    "\"\"\"\n",
    "        \n",
    "        for juror in result.get('individual_jurors', []):\n",
    "            model_name = juror.get('model', 'Unknown')\n",
    "            juror_severity = juror.get('severity', 'N/A')\n",
    "            juror_confidence = juror.get('confidence', 'N/A')\n",
    "            juror_category = juror.get('category', 'Unknown')\n",
    "            reasoning = juror.get('reasoning', juror.get('error', 'No reasoning'))\n",
    "            \n",
    "            juror_severity_class = f\"severity-{int(float(juror_severity))}\" if isinstance(juror_severity, (int, float)) else \"\"\n",
    "            \n",
    "            html += f\"\"\"\n",
    "                            <tr>\n",
    "                                <td class=\"model-name\">{model_name}</td>\n",
    "                                <td>{juror_category}</td>\n",
    "                                <td><span class=\"severity-badge {juror_severity_class}\">{juror_severity}/7</span></td>\n",
    "                                <td>{juror_confidence if isinstance(juror_confidence, str) else f'{juror_confidence:.2f}'}</td>\n",
    "                                <td>{reasoning}</td>\n",
    "                            </tr>\n",
    "\"\"\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "                        </tbody>\n",
    "                    </table>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "\"\"\"\n",
    "    \n",
    "    html += f\"\"\"\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        function filterFindings(btn) {{\n",
    "            const filter = btn.getAttribute('data-filter');\n",
    "\n",
    "            // Update active button\n",
    "            document.querySelectorAll('.filter-btn').forEach(b => b.classList.remove('active'));\n",
    "            btn.classList.add('active');\n",
    "\n",
    "            // Show/hide findings\n",
    "            const findings = document.querySelectorAll('.finding');\n",
    "            let visibleCount = 0;\n",
    "            findings.forEach(f => {{\n",
    "                if (filter === 'all' || f.getAttribute('data-category') === filter) {{\n",
    "                    f.classList.remove('hidden');\n",
    "                    visibleCount++;\n",
    "                }} else {{\n",
    "                    f.classList.add('hidden');\n",
    "                    // Collapse hidden findings to avoid stale open state\n",
    "                    f.classList.remove('expanded');\n",
    "                }}\n",
    "            }});\n",
    "\n",
    "            document.getElementById('filter-count').textContent =\n",
    "                visibleCount + ' of {len(results)} shown';\n",
    "        }}\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "    \n",
    "    with open(output_html, 'w', encoding='utf-8') as f:\n",
    "        f.write(html)\n",
    "    \n",
    "    print(f\"Report generated: {output_html}\")\n",
    "    print(f\"{len(results)} findings analyzed\")\n",
    "    print(f\"Average severity: {avg_severity:.2f}/7\")\n",
    "    print(f\"High severity (>=5): {high_severity_count}\")\n",
    "    print(f\"Flagged for review: {flagged_count}\")\n",
    "    \n",
    "    return output_html\n",
    "\n",
    "generate_html_report(OUTPUT_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
